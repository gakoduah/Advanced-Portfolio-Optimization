{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89518158",
   "metadata": {},
   "source": [
    "# Advanced Portfolio Optimization System\n",
    "\n",
    "## Objective\n",
    "The project's objective is to create a complete, data-driven system for portfolio optimization that builds the best possible investment portfolios by utilizing machine learning, risk management strategies, and contemporary quantitative finance methodologies. The system aims to:\n",
    "\n",
    "1. **Maximize risk-adjusted returns** using mean-variance optimization (efficient frontier, minimum variance, and maximum Sharpe ratio portfolios)\n",
    "2. **Enhance risk modeling** through Extreme Value Theory (EVT), copula-based simulations, and tail dependence analysis\n",
    "3. **Incorporate machine learning** (LSTM networks and Random Forests) to improve covariance matrix estimation and return predictions\n",
    "4. **Provide dynamic risk management** with stress testing, regime detection, and position sizing adjustments\n",
    "5. **Deliver intuitive visualizations** for portfolio performance, risk metrics, and optimization results\n",
    "\n",
    "## Relevance & Motivation\n",
    "Portfolio optimization is a fundamental challenge in finance, balancing the trade-off between risk and return. Traditional methods like Markowitz's mean-variance optimization have limitations, including:\n",
    "\n",
    "- **Sensitivity to input estimates** (expected returns and covariance matrices)\n",
    "- **Failure to account for extreme market events** (fat tails, skewness)\n",
    "- **Static assumptions** that don't adapt to changing market regimes\n",
    "\n",
    "This project addresses these challenges by:\n",
    "\n",
    "✅ **Improving robustness** with ML-enhanced covariance estimation  \n",
    "✅ **Capturing tail risks** using EVT and copula models  \n",
    "✅ **Adapting to market conditions** via regime detection and stress testing  \n",
    "✅ **Providing actionable insights** through interactive dashboards  \n",
    "\n",
    "## Methodology Overview\n",
    "The system integrates:\n",
    "\n",
    "1. **Core Optimization**  \n",
    "   - Efficient frontier generation  \n",
    "   - Minimum variance and maximum Sharpe portfolios  \n",
    "   - Monte Carlo simulations for portfolio diversification  \n",
    "\n",
    "2. **Machine Learning Enhancements**  \n",
    "   - LSTM networks for return forecasting  \n",
    "   - Random Forests for dynamic covariance prediction  \n",
    "   - Walk-forward backtesting for strategy validation  \n",
    "\n",
    "3. **Advanced Risk Modeling**  \n",
    "   - Extreme Value Theory (VaR & CVaR)  \n",
    "   - Gaussian and Student-t copulas for dependency modeling  \n",
    "   - Tail dependence network analysis  \n",
    "\n",
    "4. **Practical Applications**  \n",
    "   - Scenario stress testing (2008 Crisis, COVID-19, Inflation Shock)  \n",
    "   - Dynamic position sizing based on risk thresholds  \n",
    "   - Market regime detection using Gaussian Mixture Models  \n",
    "\n",
    "## Expected Outcomes\n",
    "By the end of this analysis, we will have:\n",
    "\n",
    "- **Optimal portfolio allocations** for different risk preferences  \n",
    "- **Comprehensive risk metrics** (VaR, CVaR, tail dependence)  \n",
    "- **ML-enhanced forecasts** improving traditional methods  \n",
    "- **Interactive dashboards** for decision-making  \n",
    "\n",
    "This system is designed for **quantitative analysts, portfolio managers, and algorithmic traders** seeking a modern, data-driven approach to portfolio construction and risk management.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db250498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading stock data...\n",
      "Data successfully downloaded and processed.\n",
      "Calculating efficient frontier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:10<00:00,  9.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Monte Carlo simulation with 10000 portfolios...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:05<00:00, 1973.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ML models for covariance prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:15<00:00, 13.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ML backtesting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ML models for covariance prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:35<05:20, 35.64s/it]\u001b[A\n",
      " 40%|████      | 4/10 [00:38<00:45,  7.61s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [01:10<01:11, 14.36s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [01:14<00:45, 11.29s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [01:17<00:13,  6.93s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [01:32<00:08,  8.82s/it]\u001b[A\n",
      "100%|██████████| 10/10 [01:36<00:00,  9.67s/it]\u001b[A\n",
      "  5%|▌         | 1/19 [01:38<29:36, 98.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ML models for covariance prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:39<05:55, 39.53s/it]\u001b[A\n",
      " 30%|███       | 3/10 [00:40<01:13, 10.48s/it]\u001b[A\n",
      " 40%|████      | 4/10 [00:42<00:45,  7.56s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [01:13<01:16, 15.26s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [01:16<00:45, 11.46s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [01:21<00:28,  9.47s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [01:39<00:09,  9.22s/it]\u001b[A\n",
      "100%|██████████| 10/10 [01:40<00:00, 10.05s/it]\u001b[A\n",
      " 11%|█         | 2/19 [03:21<28:34, 100.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ML models for covariance prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:36<05:32, 36.91s/it]\u001b[A\n",
      " 30%|███       | 3/10 [00:37<01:08,  9.82s/it]\u001b[A\n",
      " 40%|████      | 4/10 [00:40<00:44,  7.39s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [01:14<01:21, 16.26s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [01:15<00:45, 11.42s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [01:17<00:25,  8.41s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [01:17<00:11,  5.87s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [01:34<00:09,  9.23s/it]\u001b[A\n",
      "100%|██████████| 10/10 [01:36<00:00,  9.66s/it]\u001b[A\n",
      " 16%|█▌        | 3/19 [04:59<26:34, 99.68s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ML models for covariance prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:36<05:32, 36.98s/it]\u001b[A\n",
      " 20%|██        | 2/10 [00:39<02:13, 16.65s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [01:20<01:12, 14.58s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [01:20<00:26,  8.69s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [01:31<00:26,  8.69s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [01:37<00:08,  8.61s/it]\u001b[A\n",
      "100%|██████████| 10/10 [01:39<00:00,  9.99s/it]\u001b[A\n",
      " 21%|██        | 4/19 [06:41<25:07, 100.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ML models for covariance prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:42<06:26, 42.99s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [01:11<01:02, 12.51s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [01:14<00:41, 10.35s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [01:22<00:29,  9.71s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [01:35<00:08,  8.36s/it]\u001b[A\n",
      "100%|██████████| 10/10 [01:37<00:00,  9.76s/it]\u001b[A\n",
      " 26%|██▋       | 5/19 [08:20<23:19, 99.95s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ML models for covariance prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:43<06:30, 43.36s/it]\u001b[A\n",
      " 20%|██        | 2/10 [00:43<02:23, 17.91s/it]\u001b[A\n",
      " 30%|███       | 3/10 [00:45<01:13, 10.50s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [01:14<01:04, 12.93s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [01:21<00:44, 11.12s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [01:26<00:28,  9.40s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [01:38<00:07,  7.85s/it]\u001b[A\n",
      "100%|██████████| 10/10 [01:41<00:00, 10.10s/it]\u001b[A\n",
      " 32%|███▏      | 6/19 [10:02<21:52, 100.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ML models for covariance prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:35<05:22, 35.78s/it]\u001b[A\n",
      " 20%|██        | 2/10 [00:36<02:02, 15.28s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [01:12<01:04, 12.82s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [01:16<00:25,  8.39s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [01:21<00:15,  7.83s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [01:33<00:08,  8.86s/it]\u001b[A\n",
      "100%|██████████| 10/10 [01:41<00:00, 10.12s/it]\u001b[A\n",
      " 37%|███▋      | 7/19 [11:45<20:18, 101.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ML models for covariance prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:41<06:14, 41.66s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [01:05<00:56, 11.36s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [01:16<00:44, 11.20s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [01:19<00:27,  9.20s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [01:28<00:07,  7.26s/it]\u001b[A\n",
      "100%|██████████| 10/10 [01:33<00:00,  9.35s/it]\u001b[A\n",
      " 42%|████▏     | 8/19 [13:20<18:13, 99.40s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ML models for covariance prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:35<05:15, 35.05s/it]\u001b[A\n",
      " 20%|██        | 2/10 [00:35<01:58, 14.82s/it]\u001b[A\n",
      " 30%|███       | 3/10 [00:38<01:05,  9.29s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [01:18<01:16, 15.30s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [01:19<00:25,  8.64s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [01:21<00:14,  7.11s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [01:37<00:09,  9.51s/it]\u001b[A\n",
      "100%|██████████| 10/10 [01:39<00:00, 10.00s/it]\u001b[A\n",
      " 47%|████▋     | 9/19 [15:02<16:41, 100.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ML models for covariance prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:41<06:13, 41.52s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [01:14<01:05, 13.15s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [01:29<00:54, 13.56s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [01:30<00:31, 10.52s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [01:45<00:09,  9.30s/it]\u001b[A\n",
      "100%|██████████| 10/10 [01:49<00:00, 10.97s/it]\u001b[A\n",
      " 53%|█████▎    | 10/19 [16:53<15:32, 103.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ML models for covariance prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:47<07:09, 47.73s/it]\u001b[A\n",
      " 20%|██        | 2/10 [00:56<03:16, 24.57s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [01:40<01:27, 17.52s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [01:54<01:06, 16.65s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [01:57<00:38, 12.99s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [02:02<00:08,  8.43s/it]\u001b[A\n",
      "100%|██████████| 10/10 [02:10<00:00, 13.01s/it]\u001b[A\n",
      " 58%|█████▊    | 11/19 [19:05<14:57, 112.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ML models for covariance prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:30<04:35, 30.66s/it]\u001b[A\n",
      " 20%|██        | 2/10 [00:31<01:45, 13.19s/it]\u001b[A\n",
      " 30%|███       | 3/10 [00:35<01:01,  8.77s/it]\u001b[A\n",
      " 40%|████      | 4/10 [00:37<00:37,  6.27s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [01:04<01:08, 13.69s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [01:09<00:24,  8.03s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [01:10<00:12,  6.14s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [01:21<00:07,  7.58s/it]\u001b[A\n",
      "100%|██████████| 10/10 [01:23<00:00,  8.38s/it]\u001b[A\n",
      " 63%|██████▎   | 12/19 [20:30<12:07, 103.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ML models for covariance prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:39<05:53, 39.23s/it]\u001b[A\n",
      " 20%|██        | 2/10 [00:40<02:17, 17.16s/it]\u001b[A\n",
      " 40%|████      | 4/10 [00:41<00:40,  6.71s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [01:27<01:33, 18.62s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [01:27<00:51, 12.96s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [01:28<00:28,  9.42s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [01:31<00:14,  7.30s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [01:38<00:07,  7.34s/it]\u001b[A\n",
      "100%|██████████| 10/10 [01:43<00:00, 10.37s/it]\u001b[A\n",
      " 68%|██████▊   | 13/19 [22:16<10:26, 104.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ML models for covariance prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:38<05:43, 38.15s/it]\u001b[A\n",
      " 40%|████      | 4/10 [00:40<00:47,  7.93s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [01:08<01:07, 13.53s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [01:14<00:45, 11.31s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [01:29<00:07,  7.86s/it]\u001b[A\n",
      "100%|██████████| 10/10 [01:32<00:00,  9.29s/it]\u001b[A\n",
      " 74%|███████▎  | 14/19 [23:50<08:27, 101.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ML models for covariance prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:35<05:18, 35.37s/it]\u001b[A\n",
      " 20%|██        | 2/10 [00:35<01:58, 14.81s/it]\u001b[A\n",
      " 40%|████      | 4/10 [00:38<00:39,  6.50s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [01:02<00:59, 11.88s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [01:10<00:42, 10.65s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [01:11<00:22,  7.53s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [01:22<00:06,  6.72s/it]\u001b[A\n",
      "100%|██████████| 10/10 [01:27<00:00,  8.75s/it]\u001b[A\n",
      " 79%|███████▉  | 15/19 [25:19<06:30, 97.66s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ML models for covariance prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:33<05:01, 33.54s/it]\u001b[A\n",
      " 20%|██        | 2/10 [00:46<02:52, 21.54s/it]\u001b[A\n",
      " 40%|████      | 4/10 [00:48<00:50,  8.46s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [01:10<01:04, 12.82s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [01:20<00:48, 12.00s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [01:24<00:28,  9.40s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [01:37<00:08,  8.06s/it]\u001b[A\n",
      "100%|██████████| 10/10 [01:38<00:00,  9.81s/it]\u001b[A\n",
      " 84%|████████▍ | 16/19 [26:58<04:54, 98.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ML models for covariance prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:37<05:38, 37.62s/it]\u001b[A\n",
      " 40%|████      | 4/10 [00:38<00:43,  7.29s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [01:06<01:05, 13.03s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [01:11<00:43, 10.78s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [01:13<00:25,  8.35s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [01:17<00:14,  7.01s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [01:32<00:09,  9.34s/it]\u001b[A\n",
      "100%|██████████| 10/10 [01:32<00:00,  9.26s/it]\u001b[A\n",
      " 89%|████████▉ | 17/19 [28:32<03:13, 96.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ML models for covariance prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:30<04:34, 30.53s/it]\u001b[A\n",
      " 20%|██        | 2/10 [00:36<02:10, 16.34s/it]\u001b[A\n",
      " 30%|███       | 3/10 [00:38<01:06,  9.48s/it]\u001b[A\n",
      " 40%|████      | 4/10 [00:39<00:37,  6.28s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [01:01<00:58, 11.77s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [01:07<00:39,  9.81s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [01:15<00:27,  9.24s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [01:21<00:06,  6.18s/it]\u001b[A\n",
      "100%|██████████| 10/10 [01:23<00:00,  8.38s/it]\u001b[A\n",
      " 95%|█████████▍| 18/19 [29:57<01:33, 93.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ML models for covariance prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:31<04:47, 31.94s/it]\u001b[A\n",
      " 20%|██        | 2/10 [00:35<02:04, 15.51s/it]\u001b[A\n",
      " 30%|███       | 3/10 [00:36<01:01,  8.77s/it]\u001b[A\n",
      " 40%|████      | 4/10 [00:38<00:36,  6.08s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [01:12<01:20, 16.03s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [01:17<00:49, 12.49s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [01:18<00:13,  6.52s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [01:33<00:08,  8.68s/it]\u001b[A\n",
      "100%|██████████| 10/10 [01:34<00:00,  9.50s/it]\u001b[A\n",
      "100%|██████████| 19/19 [31:34<00:00, 99.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating efficient frontier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:10<00:00,  9.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded models from cache\n",
      "Starting model retraining...\n",
      "\n",
      "================================================================================\n",
      "Portfolio Optimization Results\n",
      "================================================================================\n",
      "\n",
      "Minimum Variance Portfolio:\n",
      "AAPL     0.01%\n",
      "MSFT    12.01%\n",
      "GOOG     3.89%\n",
      "AMZN    66.88%\n",
      "META     3.78%\n",
      "JPM      0.00%\n",
      "JNJ      0.00%\n",
      "XOM      0.00%\n",
      "TSLA     0.45%\n",
      "NVDA    12.98%\n",
      "dtype: object\n",
      "\n",
      "Maximum Sharpe Portfolio:\n",
      "AAPL    16.96%\n",
      "MSFT     0.00%\n",
      "GOOG     0.00%\n",
      "AMZN     0.00%\n",
      "META     0.00%\n",
      "JPM      0.00%\n",
      "JNJ     20.33%\n",
      "XOM     32.11%\n",
      "TSLA    30.60%\n",
      "NVDA     0.00%\n",
      "dtype: object\n",
      "\n",
      "ML Minimum Variance Portfolio:\n",
      "AAPL    10.38%\n",
      "MSFT     8.27%\n",
      "GOOG    10.70%\n",
      "AMZN    25.69%\n",
      "META    11.31%\n",
      "JPM      5.76%\n",
      "JNJ     11.57%\n",
      "XOM      3.93%\n",
      "TSLA     2.53%\n",
      "NVDA     9.85%\n",
      "dtype: object\n",
      "\n",
      "Risk Metrics:\n",
      "EVT VaR: 0.0796\n",
      "Position Size: 100.00%\n",
      "YF.download() has changed argument auto_adjust default to True\n",
      "\n",
      "Outputs saved:\n",
      "- weights.csv\n",
      "- risk_dashboard.html\n",
      "- optimization_dashboard.html\n",
      "- tail_dependence.png\n",
      "\n",
      "System running...\n",
      "Retraining complete. Sleeping for 24 hours.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import genextreme, norm, t\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from copulae import GaussianCopula, StudentCopula\n",
    "from copulae.core import pseudo_obs\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import networkx as nx\n",
    "import time\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from joblib import dump, load\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.config.optimizer.set_jit(True)  \n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "# ===============================\n",
    "# Configuration Parameters\n",
    "# ===============================\n",
    "RISK_FREE_RATE = 0.06  \n",
    "START_DATE = '2018-01-01'\n",
    "END_DATE = '2023-12-31'\n",
    "STOCKS = ['AAPL', 'MSFT', 'GOOG', 'AMZN', 'META',\n",
    "          'JPM', 'JNJ', 'XOM', 'TSLA', 'NVDA']\n",
    "SEQ_LENGTH = 20  # For LSTM models\n",
    "N_SIMULATIONS = 10000  # For Monte Carlo\n",
    "CONFIDENCE_LEVEL = 0.95  # For VaR calculations\n",
    "SCENARIOS = {\n",
    "    '2008 Crisis': {'cov_multiplier': 3.5, 'return_multiplier': 0.4},\n",
    "    '2020 COVID': {'cov_multiplier': 2.8, 'return_multiplier': 0.6},\n",
    "    'Inflation Shock': {'cov_multiplier': 2.0, 'return_multiplier': 0.7}\n",
    "}\n",
    "\n",
    "# ===============================\n",
    "# Data Collection Module\n",
    "# ===============================\n",
    "def download_data():\n",
    "    \"\"\"Fetch and prepare historical stock data\"\"\"\n",
    "    print(\"Downloading stock data...\")\n",
    "    try:\n",
    "        data = yf.download(STOCKS, start=START_DATE, end=END_DATE, progress=False, auto_adjust=True)['Close']\n",
    "        returns = data.pct_change().dropna()\n",
    "        annual_returns = returns.mean() * 252\n",
    "        cov_matrix = returns.cov() * 252\n",
    "        print(\"Data successfully downloaded and processed.\")\n",
    "        return data, returns, annual_returns, cov_matrix\n",
    "    except Exception as e:\n",
    "        print(f\"Data download failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# ===============================\n",
    "# Portfolio Optimization Core\n",
    "# ===============================\n",
    "def portfolio_metrics(weights, returns, cov_matrix):\n",
    "    \"\"\"Calculate portfolio return, volatility and Sharpe ratio\"\"\"\n",
    "    port_return = np.dot(weights, returns)\n",
    "    port_vol = np.sqrt(weights.T @ cov_matrix @ weights)\n",
    "    sharpe = (port_return - RISK_FREE_RATE) / port_vol\n",
    "    return port_return, port_vol, sharpe\n",
    "\n",
    "def min_variance(cov_matrix):\n",
    "    \"\"\"Minimum variance portfolio optimization\"\"\"\n",
    "    n = cov_matrix.shape[0]\n",
    "    init_guess = np.repeat(1/n, n)\n",
    "    bounds = ((0, 1),) * n\n",
    "    constraints = {'type': 'eq', 'fun': lambda x: np.sum(x) - 1}\n",
    "\n",
    "    result = minimize(lambda w: w.T @ cov_matrix @ w,\n",
    "                      init_guess,\n",
    "                      method='SLSQP',\n",
    "                      bounds=bounds,\n",
    "                      constraints=constraints)\n",
    "\n",
    "    return result.x\n",
    "\n",
    "def max_sharpe(returns, cov_matrix):\n",
    "    \"\"\"Maximum Sharpe ratio portfolio optimization\"\"\"\n",
    "    n = len(returns)\n",
    "    init_guess = np.repeat(1/n, n)\n",
    "    bounds = ((0, 1),) * n\n",
    "    constraints = {'type': 'eq', 'fun': lambda x: np.sum(x) - 1}\n",
    "\n",
    "    def negative_sharpe(w):\n",
    "        ret = np.dot(w, returns)\n",
    "        vol = np.sqrt(w.T @ cov_matrix @ w)\n",
    "        return -(ret - RISK_FREE_RATE) / vol\n",
    "\n",
    "    result = minimize(negative_sharpe,\n",
    "                      init_guess,\n",
    "                      method='SLSQP',\n",
    "                      bounds=bounds,\n",
    "                      constraints=constraints)\n",
    "\n",
    "    return result.x\n",
    "\n",
    "def efficient_frontier(returns, cov_matrix, num_points=100):\n",
    "    \"\"\"Calculate efficient frontier\"\"\"\n",
    "    min_ret = returns.min()\n",
    "    max_ret = returns.max()\n",
    "    target_rets = np.linspace(min_ret, max_ret, num_points)\n",
    "    volatilities = []\n",
    "    print(\"Calculating efficient frontier...\")\n",
    "    \n",
    "    init_guess = np.repeat(1/len(returns), len(returns))\n",
    "    bounds = tuple((0, 1) for _ in range(len(returns)))\n",
    "    \n",
    "    for ret in tqdm(target_rets):\n",
    "        constraints = (\n",
    "            {'type': 'eq', 'fun': lambda x: np.sum(x) - 1},\n",
    "            {'type': 'eq', 'fun': lambda x: np.dot(x, returns) - ret}\n",
    "        )\n",
    "        result = minimize(lambda w: w.T @ cov_matrix @ w,\n",
    "                         init_guess,\n",
    "                         method='SLSQP',\n",
    "                         bounds=bounds,\n",
    "                         constraints=constraints)\n",
    "        if result.success:\n",
    "            volatilities.append(np.sqrt(result.fun))\n",
    "        else:\n",
    "            volatilities.append(np.nan)\n",
    "\n",
    "    return target_rets, volatilities\n",
    "\n",
    "def monte_carlo_simulation(returns, cov_matrix, n_portfolios=10000):\n",
    "    \"\"\"Monte Carlo simulation of random portfolios\"\"\"\n",
    "    results = np.zeros((n_portfolios, 3))\n",
    "    weights_record = []\n",
    "\n",
    "    print(f\"Running Monte Carlo simulation with {n_portfolios} portfolios...\")\n",
    "    for i in tqdm(range(n_portfolios)):\n",
    "        w = np.random.dirichlet(np.ones(len(returns)))\n",
    "        ret, vol, sharpe = portfolio_metrics(w, returns, cov_matrix)\n",
    "        results[i] = [ret, vol, sharpe]\n",
    "        weights_record.append(w)\n",
    "\n",
    "    return results, np.array(weights_record)\n",
    "\n",
    "# ===============================\n",
    "# Optimized Machine Learning Integration\n",
    "# ===============================\n",
    "def create_lstm_model(seq_length):\n",
    "    \"\"\"Create optimized LSTM model with fixed parameters\"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=(seq_length, 1), batch_size=32),  \n",
    "        LSTM(64, return_sequences=True),\n",
    "        Dropout(0.3),\n",
    "        LSTM(32),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', run_eagerly=False)\n",
    "    return model\n",
    "\n",
    "def train_stock_model(stock, data, returns, horizon):\n",
    "    \"\"\"Train model for a single stock with fixed parameters\"\"\"\n",
    "    try:\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - SEQ_LENGTH - horizon):\n",
    "            X.append(data[i:i+SEQ_LENGTH])\n",
    "            y.append(data[i+SEQ_LENGTH:i+SEQ_LENGTH+horizon].mean())\n",
    "\n",
    "        if len(X) < 10:\n",
    "            return stock, None, None, None, None\n",
    "\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "        model = create_lstm_model(SEQ_LENGTH)\n",
    "        early_stop = EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)\n",
    "        model.fit(X, y, epochs=30, batch_size=32, verbose=0, callbacks=[early_stop])\n",
    "\n",
    "        preds = model.predict(X, verbose=0).flatten()\n",
    "        valid_dates = returns.index[SEQ_LENGTH+horizon-1:SEQ_LENGTH+horizon-1+len(preds)]\n",
    "        residuals = y - preds\n",
    "\n",
    "        last_seq = data[-SEQ_LENGTH:].reshape(1, SEQ_LENGTH, 1)\n",
    "        pred_return = model.predict(last_seq, verbose=0)[0][0]\n",
    "        \n",
    "        return stock, model, residuals, pred_return, valid_dates\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {stock}: {str(e)}\")\n",
    "        return stock, None, None, None, None\n",
    "\n",
    "def ml_based_covariance(returns, horizon=21):\n",
    "    \"\"\"Parallelized ML-enhanced covariance prediction\"\"\"\n",
    "    print(\"Training ML models for covariance prediction...\")\n",
    "    predicted_returns = pd.Series(index=returns.columns, dtype=np.float64)\n",
    "    residuals = pd.DataFrame(index=returns.index[SEQ_LENGTH+horizon-1:],\n",
    "                           columns=returns.columns)\n",
    "    models = {}\n",
    "\n",
    "    # Parallel training\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        futures = []\n",
    "        for stock in returns.columns:\n",
    "            data = returns[stock].values\n",
    "            futures.append(executor.submit(train_stock_model, stock, data, returns, horizon))\n",
    "        \n",
    "        for future in tqdm(futures, total=len(returns.columns)):\n",
    "            stock, model, res, pred, dates = future.result()\n",
    "            if model is not None:\n",
    "                models[stock] = model\n",
    "                predicted_returns[stock] = pred\n",
    "                if res is not None and dates is not None:\n",
    "                    residuals.loc[dates, stock] = res\n",
    "\n",
    "    if not models:\n",
    "        raise ValueError(\"No valid models trained - check input data\")\n",
    "\n",
    "    residuals = residuals.dropna()\n",
    "    scaler = StandardScaler()\n",
    "    scaled_residuals = pd.DataFrame(scaler.fit_transform(residuals),\n",
    "                                  columns=residuals.columns,\n",
    "                                  index=residuals.index)\n",
    "\n",
    "    cov_features = []\n",
    "    cov_target = []\n",
    "\n",
    "    for i in range(len(scaled_residuals) - horizon):\n",
    "        current = scaled_residuals.iloc[i:i+horizon]\n",
    "        cov_features.append(current.values.flatten())\n",
    "        future = returns.loc[scaled_residuals.index[i+1:i+1+horizon]].cov().values.flatten()\n",
    "        cov_target.append(future)\n",
    "\n",
    "    if len(cov_features) < 10:\n",
    "        raise ValueError(\"Insufficient data for covariance prediction\")\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators=100,\n",
    "                             max_depth=5,\n",
    "                             max_features='sqrt',\n",
    "                             random_state=42)\n",
    "    rf.fit(cov_features, cov_target)\n",
    "\n",
    "    last_residuals = scaled_residuals.iloc[-horizon:].values.flatten().reshape(1, -1)\n",
    "    future_cov = rf.predict(last_residuals)[0]\n",
    "    cov_matrix = future_cov.reshape(len(returns.columns), len(returns.columns))\n",
    "    cov_matrix = (cov_matrix + cov_matrix.T) / 2\n",
    "    np.fill_diagonal(cov_matrix, returns.var() * 252)\n",
    "\n",
    "    return predicted_returns * 252, pd.DataFrame(cov_matrix,\n",
    "                                               index=returns.columns,\n",
    "                                               columns=returns.columns)\n",
    "\n",
    "# ===============================\n",
    "# Advanced Risk Modeling\n",
    "# ===============================\n",
    "def calculate_evt(portfolio_returns):\n",
    "    \"\"\"Extreme Value Theory risk metrics\"\"\"\n",
    "    losses = -portfolio_returns\n",
    "    threshold = np.quantile(losses, 0.95)\n",
    "    excess_losses = losses[losses > threshold] - threshold\n",
    "\n",
    "    shape, loc, scale = genextreme.fit(excess_losses)\n",
    "    evt_var = genextreme.ppf(CONFIDENCE_LEVEL, shape, loc, scale) + threshold\n",
    "    evt_es = threshold + scale/(1-shape)*((1-CONFIDENCE_LEVEL)/(1-0.95))**(-shape)\n",
    "\n",
    "    return {\n",
    "        'EVT_VaR': evt_var,\n",
    "        'EVT_CVaR': evt_es,\n",
    "        'shape': shape,\n",
    "        'scale': scale\n",
    "    }\n",
    "\n",
    "def copula_based_simulation(returns, n_simulations=10000):\n",
    "    \"\"\"Copula-based risk simulation\"\"\"\n",
    "    u = returns.rank(pct=True).values\n",
    "    u = (u - u.min()) / (u.max() - u.min())\n",
    "\n",
    "    gauss_copula = GaussianCopula(dim=returns.shape[1])\n",
    "    gauss_copula.fit(u)\n",
    "\n",
    "    t_copula = StudentCopula(dim=returns.shape[1])\n",
    "    t_copula.fit(u)\n",
    "\n",
    "    gauss_sim = gauss_copula.random(n_simulations)\n",
    "    t_sim = t_copula.random(n_simulations)\n",
    "\n",
    "    sim_returns_gauss = np.zeros_like(gauss_sim)\n",
    "    sim_returns_t = np.zeros_like(t_sim)\n",
    "\n",
    "    for i in range(returns.shape[1]):\n",
    "        ecdf = norm(loc=returns.iloc[:, i].mean(), scale=returns.iloc[:, i].std())\n",
    "        sim_returns_gauss[:, i] = ecdf.ppf(gauss_sim[:, i])\n",
    "        sim_returns_t[:, i] = ecdf.ppf(t_sim[:, i])\n",
    "\n",
    "    return sim_returns_gauss, sim_returns_t\n",
    "\n",
    "def advanced_risk_metrics(returns, weights):\n",
    "    \"\"\"Comprehensive risk analysis\"\"\"\n",
    "    weighted_returns = returns @ weights\n",
    "\n",
    "    evt_results = calculate_evt(weighted_returns)\n",
    "\n",
    "    gauss_sim, t_sim = copula_based_simulation(returns, N_SIMULATIONS)\n",
    "    portfolio_sims_gauss = gauss_sim @ weights\n",
    "    portfolio_sims_t = t_sim @ weights\n",
    "\n",
    "    def calculate_sim_risk(sims):\n",
    "        var = np.quantile(sims, 1 - CONFIDENCE_LEVEL)\n",
    "        es = sims[sims <= var].mean()\n",
    "        return var, es\n",
    "\n",
    "    gauss_var, gauss_es = calculate_sim_risk(portfolio_sims_gauss)\n",
    "    t_var, t_es = calculate_sim_risk(portfolio_sims_t)\n",
    "\n",
    "    tail_dependence = {}\n",
    "    for i in range(len(returns.columns)):\n",
    "        for j in range(i+1, len(returns.columns)):\n",
    "            pair_returns = returns.iloc[:, [i, j]]\n",
    "            upper_tail = np.corrcoef(\n",
    "                (pair_returns > pair_returns.quantile(0.95)).T\n",
    "            )[0, 1]\n",
    "            lower_tail = np.corrcoef(\n",
    "                (pair_returns < pair_returns.quantile(0.05)).T\n",
    "            )[0, 1]\n",
    "            tail_dependence[f\"{returns.columns[i]}-{returns.columns[j]}\"] = {\n",
    "                'upper': upper_tail,\n",
    "                'lower': lower_tail\n",
    "            }\n",
    "\n",
    "    return {\n",
    "        'Gaussian_Copula_VaR': gauss_var,\n",
    "        'Gaussian_Copula_CVaR': gauss_es,\n",
    "        't_Copula_VaR': t_var,\n",
    "        't_Copula_CVaR': t_es,\n",
    "        'EVT_VaR': evt_results['EVT_VaR'], \n",
    "        'EVT_CVaR': evt_results['EVT_CVaR'], \n",
    "        'Tail_Dependence': tail_dependence,\n",
    "        'shape': evt_results['shape'], \n",
    "        'scale': evt_results['scale']\n",
    "    }\n",
    "\n",
    "# ===============================\n",
    "# Real-World Applications\n",
    "# ===============================\n",
    "def calculate_metrics(returns, weights):\n",
    "    \"\"\"Portfolio performance metrics\"\"\"\n",
    "    weighted_returns = returns @ weights\n",
    "    cumulative = (1 + weighted_returns).cumprod()\n",
    "    peak = cumulative.cummax()\n",
    "    drawdown = (cumulative - peak) / peak\n",
    "    sortino_vol = weighted_returns[weighted_returns < 0].std() * np.sqrt(252)\n",
    "    sortino = (weighted_returns.mean() * 252 - RISK_FREE_RATE) / sortino_vol\n",
    "\n",
    "    return {\n",
    "        'CAGR': cumulative[-1]**(252/len(cumulative)) - 1,\n",
    "        'Max Drawdown': drawdown.min(),\n",
    "        'Sortino Ratio': sortino,\n",
    "        'VaR 95%': np.percentile(weighted_returns, 5),\n",
    "        'CVaR 95%': weighted_returns[weighted_returns <= np.percentile(weighted_returns, 5)].mean(),\n",
    "        'Skewness': weighted_returns.skew(),\n",
    "        'Kurtosis': weighted_returns.kurtosis()\n",
    "    }\n",
    "\n",
    "def ml_backtest(returns, lookback=252, horizon=63):\n",
    "    \"\"\"Walk-forward ML backtest\"\"\"\n",
    "    weights_history = []\n",
    "    actual_returns = []\n",
    "    predicted_returns = []\n",
    "\n",
    "    print(\"Running ML backtesting...\")\n",
    "    for i in tqdm(range(lookback, len(returns) - horizon, horizon)):\n",
    "        train_data = returns.iloc[i-lookback:i]\n",
    "\n",
    "        try:\n",
    "            pred_returns, pred_cov = ml_based_covariance(train_data, horizon)\n",
    "            weights = max_sharpe(pred_returns, pred_cov)\n",
    "\n",
    "            weights_history.append(weights)\n",
    "            actual_returns.append(returns.iloc[i:i+horizon].mean().dot(weights))\n",
    "            predicted_returns.append(pred_returns.dot(weights))\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping period: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return {\n",
    "        'weights': weights_history,\n",
    "        'actual_returns': np.array(actual_returns),\n",
    "        'predicted_returns': np.array(predicted_returns)\n",
    "    }\n",
    "\n",
    "def stress_test_portfolio(weights, cov_matrix, annual_returns, scenarios):\n",
    "    \"\"\"Scenario stress testing\"\"\"\n",
    "    results = {}\n",
    "    for name, shock in scenarios.items():\n",
    "        stressed_cov = cov_matrix * shock['cov_multiplier']\n",
    "        port_vol = np.sqrt(weights.T @ stressed_cov @ weights)\n",
    "        port_return = np.dot(weights, annual_returns) * shock['return_multiplier']\n",
    "        results[name] = {\n",
    "            'Return': port_return,\n",
    "            'Volatility': port_vol,\n",
    "            'Sharpe': (port_return - RISK_FREE_RATE) / port_vol\n",
    "        }\n",
    "    return pd.DataFrame(results).T\n",
    "\n",
    "def dynamic_risk_threshold(risk_metrics):\n",
    "    \"\"\"Dynamic position sizing\"\"\"\n",
    "    current_var = risk_metrics.get('EVT_VaR', 0)\n",
    "    max_drawdown_limit = 0.15\n",
    "    position_size = min(1.0, max_drawdown_limit / abs(current_var)) if current_var != 0 else 1.0\n",
    "    return position_size\n",
    "\n",
    "def detect_market_regimes(returns, n_regimes=3):\n",
    "    \"\"\"Market regime detection\"\"\"\n",
    "    features = returns.rolling(21).agg(['mean', 'std', 'skew', 'kurtosis']).dropna()\n",
    "    gmm = GaussianMixture(n_components=n_regimes, covariance_type='full')\n",
    "    gmm.fit(features)\n",
    "    regimes = gmm.predict(features)\n",
    "\n",
    "    regime_cov = {}\n",
    "    for i in range(n_regimes):\n",
    "        regime_data = returns[regimes == i]\n",
    "        if len(regime_data) > 10:\n",
    "            regime_cov[i] = regime_data.cov()\n",
    "\n",
    "    return regimes, regime_cov\n",
    "\n",
    "# ===============================\n",
    "# Visualization System\n",
    "# ===============================\n",
    "def plot_tail_dependence(tail_dep):\n",
    "    \"\"\"Tail dependence network visualization\"\"\"\n",
    "    G = nx.Graph()\n",
    "    for pair, metrics in tail_dep.items():\n",
    "        stock1, stock2 = pair.split('-')\n",
    "        G.add_edge(stock1, stock2, upper=metrics['upper'], lower=metrics['lower'])\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    pos = nx.spring_layout(G)\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=800, node_color='skyblue')\n",
    "    nx.draw_networkx_edges(G, pos,\n",
    "                         width=[d['upper']*10 for u,v,d in G.edges(data=True)], \n",
    "                         edge_color='red', alpha=0.6)\n",
    "    nx.draw_networkx_edges(G, pos,\n",
    "                         width=[d['lower']*10 for u,v,d in G.edges(data=True)], \n",
    "                         edge_color='blue', alpha=0.6, style='dashed')\n",
    "    nx.draw_networkx_labels(G, pos, font_size=12)\n",
    "    plt.title('Tail Dependence Network')\n",
    "    plt.axis('off')\n",
    "    plt.savefig('tail_dependence.png', dpi=300)\n",
    "\n",
    "def create_risk_dashboard(portfolio_returns, weights, cov_matrix, risk_metrics):\n",
    "    \"\"\"Interactive risk dashboard\"\"\"\n",
    "    window = 63\n",
    "    rolling_var = portfolio_returns.rolling(window).apply(\n",
    "        lambda x: np.quantile(x, 1 - CONFIDENCE_LEVEL))\n",
    "    rolling_es = portfolio_returns.rolling(window).apply(\n",
    "        lambda x: x[x <= np.quantile(x, 1 - CONFIDENCE_LEVEL)].mean())\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        specs=[[{\"type\": \"scatter\"}, {\"type\": \"heatmap\"}],\n",
    "               [{\"type\": \"bar\"}, {\"type\": \"scatter\"}]],\n",
    "        subplot_titles=(\"Returns & Risk\", \"Covariance Matrix\",\n",
    "                       \"Factor Exposure\", \"Tail Risk\")\n",
    "    )\n",
    "\n",
    "    # Returns & Risk\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=portfolio_returns.index,\n",
    "        y=portfolio_returns,\n",
    "        name='Returns',\n",
    "        line=dict(color='royalblue', width=1)),\n",
    "        row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=rolling_var.index,\n",
    "        y=rolling_var,\n",
    "        name=f'{CONFIDENCE_LEVEL:.0%} VaR',\n",
    "        line=dict(color='red', width=2)),\n",
    "        row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=rolling_es.index,\n",
    "        y=rolling_es,\n",
    "        name=f'{CONFIDENCE_LEVEL:.0%} ES',\n",
    "        line=dict(color='darkred', width=2)),\n",
    "        row=1, col=1)\n",
    "\n",
    "    # Covariance Matrix\n",
    "    fig.add_trace(go.Heatmap(\n",
    "        z=cov_matrix.values,\n",
    "        x=cov_matrix.columns,\n",
    "        y=cov_matrix.index,\n",
    "        colorscale='RdBu',\n",
    "        zmid=0),\n",
    "        row=1, col=2)\n",
    "\n",
    "    # Factor Exposure\n",
    "    factors = ['Market', 'Size', 'Value', 'Momentum']\n",
    "    exposure = pd.Series(np.random.randn(len(factors)), index=factors)\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=factors,\n",
    "        y=exposure,\n",
    "        name='Exposure'),\n",
    "        row=2, col=1)\n",
    "\n",
    "    # Tail Risk\n",
    "    losses = -portfolio_returns\n",
    "    fig.add_trace(go.Histogram(\n",
    "        x=losses,\n",
    "        nbinsx=50,\n",
    "        name='losses',\n",
    "        marker_color='royalblue'),\n",
    "        row=2, col=2)\n",
    "\n",
    "    # EVT Fit\n",
    "    threshold = np.quantile(losses, 0.95)\n",
    "    x = np.linspace(threshold, losses.max(), 100)\n",
    "    pdf = genextreme.pdf(x, risk_metrics['shape'], threshold, risk_metrics['scale'])\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x,\n",
    "        y=pdf,\n",
    "        name='EVT Fit',\n",
    "        line=dict(color='red', width=2)),\n",
    "        row=2, col=2)\n",
    "\n",
    "    # VaR markers\n",
    "    for method, color in zip(['Gaussian_Copula_VaR', 't_Copula_VaR', 'EVT_VaR'],\n",
    "                           ['green', 'orange', 'purple']):\n",
    "        fig.add_vline(\n",
    "            x=risk_metrics[method],\n",
    "            line=dict(color=color, dash='dash'),\n",
    "            row=2, col=2)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Portfolio Risk Dashboard',\n",
    "        height=900,\n",
    "        template='plotly_dark')\n",
    "    fig.write_html('risk_dashboard.html')\n",
    "\n",
    "def create_optimization_dashboard(returns, annual_returns, cov_matrix,\n",
    "                                mv_weights, ms_weights, mc_results):\n",
    "    \"\"\"Optimization visualization\"\"\"\n",
    "    target_rets, target_vols = efficient_frontier(annual_returns, cov_matrix)\n",
    "    mv_ret, mv_vol, _ = portfolio_metrics(mv_weights, annual_returns, cov_matrix)\n",
    "    ms_ret, ms_vol, ms_sharpe = portfolio_metrics(ms_weights, annual_returns, cov_matrix)\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=1,\n",
    "        specs=[[{\"type\": \"scatter\"}], [{\"type\": \"bar\"}]],\n",
    "        subplot_titles=(\"Efficient Frontier\", \"Portfolio Weights\"))\n",
    "\n",
    "    # Efficient Frontier\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=target_vols,\n",
    "        y=target_rets,\n",
    "        name='Efficient Frontier',\n",
    "        line=dict(color='blue', width=2)),\n",
    "        row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=mc_results[:, 1],\n",
    "        y=mc_results[:, 0],\n",
    "        mode='markers',\n",
    "        name='Random Portfolios',\n",
    "        marker=dict(\n",
    "            color=mc_results[:, 2],\n",
    "            colorscale='Viridis',\n",
    "            size=5,\n",
    "            showscale=True,\n",
    "            colorbar=dict(title='Sharpe Ratio'))),\n",
    "        row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[mv_vol],\n",
    "        y=[mv_ret],\n",
    "        mode='markers',\n",
    "        name='Min Variance',\n",
    "        marker=dict(size=15, symbol='star', color='red')),\n",
    "        row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[ms_vol],\n",
    "        y=[ms_ret],\n",
    "        mode='markers',\n",
    "        name='Max Sharpe',\n",
    "        marker=dict(size=15, symbol='star', color='gold')),\n",
    "        row=1, col=1)\n",
    "\n",
    "    # Portfolio Weights\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=STOCKS,\n",
    "        y=mv_weights,\n",
    "        name='Min Variance',\n",
    "        marker_color='red'),\n",
    "        row=2, col=1)\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=STOCKS,\n",
    "        y=ms_weights,\n",
    "        name='Max Sharpe',\n",
    "        marker_color='gold'),\n",
    "        row=2, col=1)\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=900,\n",
    "        title_text=\"Portfolio Optimization\",\n",
    "        template='plotly_dark')\n",
    "    fig.write_html('optimization_dashboard.html')\n",
    "\n",
    "# ===============================\n",
    "# Production System\n",
    "# ===============================\n",
    "class ModelManager:\n",
    "    \"\"\"Optimized model management with caching\"\"\"\n",
    "    def __init__(self, stocks):\n",
    "        self.stocks = stocks\n",
    "        self.models = {}\n",
    "        self.lock = threading.Lock()\n",
    "        self.cache_file = 'models_cache.pkl'\n",
    "        self.load_cache()\n",
    "\n",
    "    def load_cache(self):\n",
    "        \"\"\"Load models from cache if available\"\"\"\n",
    "        try:\n",
    "            cached_models = load(self.cache_file)\n",
    "            with self.lock:\n",
    "                self.models.update(cached_models)\n",
    "            print(\"Loaded models from cache\")\n",
    "        except:\n",
    "            print(\"No cache found, starting fresh\")\n",
    "\n",
    "    def save_cache(self):\n",
    "        \"\"\"Save models to cache\"\"\"\n",
    "        with self.lock:\n",
    "            dump(self.models, self.cache_file)\n",
    "\n",
    "    def train_async(self):\n",
    "        \"\"\"Continuous optimized model training\"\"\"\n",
    "        def training_job():\n",
    "            while True:\n",
    "                try:\n",
    "                    print(\"Starting model retraining...\")\n",
    "                    new_data = yf.download(self.stocks, period='2y', progress=False)['Close']\n",
    "                    new_returns = new_data.pct_change().dropna()\n",
    "\n",
    "                    # Parallel training\n",
    "                    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "                        futures = []\n",
    "                        for stock in self.stocks:\n",
    "                            data = new_returns[stock].values\n",
    "                            futures.append(executor.submit(\n",
    "                                self._train_stock_model, stock, data, new_returns))\n",
    "                        \n",
    "                        for future in futures:\n",
    "                            stock, model = future.result()\n",
    "                            if model:\n",
    "                                with self.lock:\n",
    "                                    self.models[stock] = model\n",
    "\n",
    "                    self.save_cache()\n",
    "                    print(\"Retraining complete. Sleeping for 24 hours.\")\n",
    "                    time.sleep(86400)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Retraining failed: {str(e)}\")\n",
    "                    time.sleep(3600)\n",
    "\n",
    "        thread = threading.Thread(target=training_job, daemon=True)\n",
    "        thread.start()\n",
    "\n",
    "    def _train_stock_model(self, stock, data, returns, horizon=21):\n",
    "        \"\"\"Helper method for stock model training\"\"\"\n",
    "        try:\n",
    "            X, y = [], []\n",
    "            for i in range(len(data) - SEQ_LENGTH - horizon):\n",
    "                X.append(data[i:i+SEQ_LENGTH])\n",
    "                y.append(data[i+SEQ_LENGTH:i+SEQ_LENGTH+horizon].mean())\n",
    "\n",
    "            if len(X) < 10:\n",
    "                return stock, None\n",
    "\n",
    "            X, y = np.array(X), np.array(y)\n",
    "            X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "            with self.lock:\n",
    "                existing_model = self.models.get(stock)\n",
    "            \n",
    "            if existing_model:\n",
    "                model = existing_model\n",
    "                model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                            loss='mse')\n",
    "            else:\n",
    "                model = create_lstm_model(SEQ_LENGTH)\n",
    "\n",
    "            early_stop = EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)\n",
    "            model.fit(X, y, epochs=30, batch_size=32, verbose=0, callbacks=[early_stop])\n",
    "            return stock, model\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error training {stock}: {str(e)}\")\n",
    "            return stock, None\n",
    "\n",
    "    def predict(self, stock, data):\n",
    "        \"\"\"Thread-safe prediction\"\"\"\n",
    "        with self.lock:\n",
    "            model = self.models.get(stock)\n",
    "            if model is None:\n",
    "                return 0\n",
    "            return model.predict(data, verbose=0)[0][0]\n",
    "\n",
    "# ===============================\n",
    "# Main Execution\n",
    "# ===============================\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # 1. Data Collection\n",
    "        data, returns, annual_returns, cov_matrix = download_data()\n",
    "\n",
    "        # 2. Core Optimization\n",
    "        mv_weights = min_variance(cov_matrix)\n",
    "        ms_weights = max_sharpe(annual_returns, cov_matrix)\n",
    "\n",
    "        # 3. Efficient Frontier\n",
    "        target_rets, target_vols = efficient_frontier(annual_returns, cov_matrix)\n",
    "\n",
    "        # 4. Monte Carlo Simulation\n",
    "        mc_results, mc_weights = monte_carlo_simulation(annual_returns, cov_matrix)\n",
    "\n",
    "        # 5. ML Optimization\n",
    "        try:\n",
    "            ml_returns, ml_cov_matrix = ml_based_covariance(returns)\n",
    "            ml_mv_weights = min_variance(ml_cov_matrix)\n",
    "            ml_ms_weights = max_sharpe(ml_returns, ml_cov_matrix)\n",
    "            backtest_results = ml_backtest(returns)\n",
    "        except Exception as e:\n",
    "            print(f\"ML failed: {str(e)}\")\n",
    "            ml_mv_weights, ml_ms_weights = mv_weights.copy(), ms_weights.copy()\n",
    "\n",
    "        # 6. Risk Analysis\n",
    "        mv_risk = advanced_risk_metrics(returns, mv_weights)\n",
    "        position_size = dynamic_risk_threshold(mv_risk)\n",
    "\n",
    "        # 7. Visualization\n",
    "        portfolio_returns = returns @ mv_weights\n",
    "        plot_tail_dependence(mv_risk['Tail_Dependence'])\n",
    "        create_risk_dashboard(portfolio_returns, mv_weights, cov_matrix, mv_risk)\n",
    "        create_optimization_dashboard(returns, annual_returns, cov_matrix,\n",
    "                                    mv_weights, ms_weights, mc_results)\n",
    "\n",
    "        # 8. Production System\n",
    "        model_manager = ModelManager(STOCKS)\n",
    "        model_manager.train_async()\n",
    "\n",
    "        # 9. Results\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Portfolio Optimization Results\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        print(\"\\nMinimum Variance Portfolio:\")\n",
    "        print(pd.Series(mv_weights, index=STOCKS).apply(lambda x: f\"{x:.2%}\"))\n",
    "\n",
    "        print(\"\\nMaximum Sharpe Portfolio:\")\n",
    "        print(pd.Series(ms_weights, index=STOCKS).apply(lambda x: f\"{x:.2%}\"))\n",
    "\n",
    "        print(\"\\nML Minimum Variance Portfolio:\")\n",
    "        print(pd.Series(ml_mv_weights, index=STOCKS).apply(lambda x: f\"{x:.2%}\"))\n",
    "\n",
    "        print(\"\\nRisk Metrics:\")\n",
    "        print(f\"EVT VaR: {mv_risk['EVT_VaR']:.4f}\")\n",
    "        print(f\"Position Size: {position_size:.2%}\")\n",
    "\n",
    "        # Save outputs\n",
    "        pd.DataFrame({\n",
    "            'Stock': STOCKS,\n",
    "            'MV': mv_weights,\n",
    "            'MS': ms_weights,\n",
    "            'ML_MV': ml_mv_weights\n",
    "        }).to_csv('weights.csv', index=False)\n",
    "\n",
    "        print(\"\\nOutputs saved:\")\n",
    "        print(\"- weights.csv\")\n",
    "        print(\"- risk_dashboard.html\")\n",
    "        print(\"- optimization_dashboard.html\")\n",
    "        print(\"- tail_dependence.png\")\n",
    "\n",
    "        print(\"\\nSystem running...\")\n",
    "        while True:\n",
    "            time.sleep(3600)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fatal error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e2ca0c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Key Achievements\n",
    "Based on the optimization results and risk metrics, we have successfully achieved all project objectives:\n",
    "\n",
    "✅ **Optimal Portfolio Construction**  \n",
    "- Developed three distinct portfolio strategies:\n",
    "  - *Minimum Variance*: 66.88% AMZN, 12.98% NVDA (lowest risk)\n",
    "  - *Max Sharpe*: 32.11% XOM, 30.60% TSLA, 20.33% JNJ (best risk-adjusted returns)\n",
    "  - *ML-Enhanced*: Balanced allocation (25.69% AMZN, 11.57% JNJ)\n",
    "\n",
    "✅ **Advanced Risk Management**  \n",
    "- Calculated **EVT VaR of 0.0796** (7.96% daily risk at 95% confidence)\n",
    "- Achieved **100% position sizing** indicating acceptable risk levels\n",
    "- Generated tail dependence network showing asset correlations\n",
    "\n",
    "✅ **Machine Learning Integration**  \n",
    "- LSTM models successfully predicted returns for covariance estimation\n",
    "- Random Forest improved covariance matrix predictions\n",
    "- ML portfolio showed more balanced allocations than classical methods\n",
    "\n",
    "✅ **Practical Applications**  \n",
    "- Stress testing scenarios implemented (2008 Crisis, COVID-19, Inflation Shock)\n",
    "- Dynamic position sizing based on real-time risk thresholds\n",
    "- Market regime detection operational\n",
    "\n",
    "### Insights from Results\n",
    "1. **Classical vs ML Approaches**  \n",
    "   - Traditional min-variance concentrated in AMZN/NVDA  \n",
    "   - ML portfolio provided better diversification (no zero-weight assets)\n",
    "\n",
    "2. **Risk-Return Tradeoffs**  \n",
    "   - Max Sharpe portfolio favored high-momentum stocks and defensive  \n",
    "   - EVT VaR confirms moderate tail risk exposure\n",
    "\n",
    "3. **Implementation Readiness**  \n",
    "   - System automatically retrains models every 24 hours  \n",
    "   - All outputs (weights, dashboards, risk metrics) production-ready\n",
    "\n",
    "### Recommendations\n",
    "1. **For Conservative Investors**  \n",
    "   - Use minimum variance portfolio (lowest volatility)  \n",
    "   - Monitor AMZN concentration risk\n",
    "\n",
    "2. **For Active Managers**  \n",
    "   - Combine ML and Max Sharpe portfolios  \n",
    "   - Utilize regime detection for dynamic adjustments\n",
    "\n",
    "3. **Next Steps**  \n",
    "   - Use alternative data sources  \n",
    "   - Test on broader asset universe (bonds, commodities)  \n",
    "   - Implement transaction cost modeling\n",
    "\n",
    "With the integration of contemporary machine learning methods with traditional finance theory, this system now offers a strong foundation for data-driven portfolio management, all the while preserving interpretability through extensive visualization capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e46f5f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
